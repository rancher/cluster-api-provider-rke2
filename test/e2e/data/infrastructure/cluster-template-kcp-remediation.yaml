apiVersion: v1
data:
  value: |-
    # generated by kind
    global
      log /dev/log local0
      log /dev/log local1 notice
      daemon
      # limit memory usage to approximately 18 MB
      # (see https://github.com/kubernetes-sigs/kind/pull/3115)
      maxconn 100000

    resolvers docker
      nameserver dns 127.0.0.11:53

    defaults
      log global
      mode tcp
      option dontlognull
      # TODO: tune these
      timeout connect 5000
      timeout client 50000
      timeout server 50000
      # allow to boot despite dns don't resolve backends
      default-server init-addr none

    frontend stats
      mode http
      bind *:8404
      stats enable
      stats uri /stats
      stats refresh 1s
      stats admin if TRUE

    frontend control-plane
      bind *:{{ .FrontendControlPlanePort }}
      {{ if .IPv6 -}}
      bind :::{{ .FrontendControlPlanePort }};
      {{- end }}
      default_backend kube-apiservers

    backend kube-apiservers
      option httpchk GET /healthz

      {{range $server, $backend := .BackendServers}}
      server {{ $server }} {{ JoinHostPort $backend.Address $.BackendControlPlanePort }} check check-ssl verify none resolvers docker resolve-prefer {{ if $.IPv6 -}} ipv6 {{- else -}} ipv4 {{- end }}
      {{- end}}

    frontend rke2-join
      bind *:9345
      {{ if .IPv6 -}}
      bind :::9345;
      {{- end }}
      default_backend rke2-servers

    backend rke2-servers
      option httpchk GET /v1-rke2/readyz
      http-check expect status 403
      {{range $server, $backend := .BackendServers}}
      server {{ $server }} {{ $backend.Address }}:9345 check check-ssl verify none
      {{- end}}
kind: ConfigMap
metadata:
  name: ${CLUSTER_NAME}-lb-config
---
apiVersion: cluster.x-k8s.io/v1beta2
kind: Cluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 10.45.0.0/16
    services:
      cidrBlocks:
        - 10.46.0.0/16
    serviceDomain: cluster.local
  controlPlaneRef:
    apiGroup: controlplane.cluster.x-k8s.io
    kind: RKE2ControlPlane
    name: ${CLUSTER_NAME}-control-plane
  infrastructureRef:
    apiGroup: infrastructure.cluster.x-k8s.io
    kind: DockerCluster
    name: ${CLUSTER_NAME}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: DockerCluster
metadata:
  name: ${CLUSTER_NAME}
spec:
  loadBalancer:
    customHAProxyConfigTemplateRef:
      name: ${CLUSTER_NAME}-lb-config
---
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: RKE2ControlPlane
metadata:
  name: ${CLUSTER_NAME}-control-plane
spec:
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  version: ${KUBERNETES_VERSION}+rke2r1
  preRKE2Commands:
    - ./wait-signal.sh "${TOKEN}" "${SERVER}" "${NAMESPACE}"
  files:
    - path: /wait-signal.sh
      content: |
        #!/bin/bash

        set -o errexit
        set -o pipefail
          
        echo "Waiting for signal..."
          
        TOKEN=$1
        SERVER=$2
        NAMESPACE=$3
          
        while true;
        do
          sleep 1s

          signal=$(curl -k -s --header "Authorization: Bearer $TOKEN" $SERVER/api/v1/namespaces/$NAMESPACE/configmaps/mhc-test | jq -r .data.signal?)
          echo "signal $signal"

          if [ "$signal" == "pass" ]; then
             curl -k -s --header "Authorization: Bearer $TOKEN" -XPATCH -H "Content-Type: application/strategic-merge-patch+json" --data '{"data": {"signal": "ack-pass"}}' $SERVER/api/v1/namespaces/$NAMESPACE/configmaps/mhc-test
             exit 0
          fi
        done
      permissions: "0777"
  rolloutStrategy:
    type: "RollingUpdate"
    rollingUpdate:
      maxSurge: 1
  gzipUserData: false
  serverConfig:
    disableComponents:
      pluginComponents:
        - rke2-ingress-nginx
      kubernetesComponents:
        - cloudController
    kubeAPIServer:
      extraArgs:
        - --anonymous-auth=true
  machineTemplate:
    spec:
      infrastructureRef:
        apiGroup: infrastructure.cluster.x-k8s.io
        kind: DockerMachineTemplate
        name: "${CLUSTER_NAME}-control-plane"
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: DockerMachineTemplate
metadata:
  name: "${CLUSTER_NAME}-control-plane"
  labels:
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
spec:
  template:
    spec:
      customImage: kindest/node:${KIND_IMAGE_VERSION}
      bootstrapTimeout: 15m
---
apiVersion: cluster.x-k8s.io/v1beta2
kind: MachineDeployment
metadata:
  name: ${CLUSTER_NAME}-mhc-0
spec:
  clusterName: ${CLUSTER_NAME}
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels:
      cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
  template:
    spec:
      version: ${KUBERNETES_VERSION}+rke2r1
      clusterName: ${CLUSTER_NAME}
      bootstrap:
        configRef:
          apiGroup: bootstrap.cluster.x-k8s.io
          kind: RKE2ConfigTemplate
          name: ${CLUSTER_NAME}-mhc-0
      infrastructureRef:
        apiGroup: infrastructure.cluster.x-k8s.io
        kind: DockerMachineTemplate
        name: ${CLUSTER_NAME}-mhc-0
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: DockerMachineTemplate
metadata:
  name: ${CLUSTER_NAME}-mhc-0
  labels:
    cluster.x-k8s.io/cluster-name: ${CLUSTER_NAME}
spec:
  template:
    spec:
      customImage: kindest/node:${KIND_IMAGE_VERSION}
      bootstrapTimeout: 15m
---
apiVersion: bootstrap.cluster.x-k8s.io/v1beta2
kind: RKE2ConfigTemplate
metadata:
  name: ${CLUSTER_NAME}-mhc-0
spec:
  template:
    spec:
      gzipUserData: false
---
apiVersion: cluster.x-k8s.io/v1beta2
kind: MachineHealthCheck
metadata:
  name: ${CLUSTER_NAME}-mhc-0
  namespace: ${NAMESPACE}
spec:
  clusterName: ${CLUSTER_NAME}
  remediation:
    triggerIf:
      unhealthyLessThanOrEqualTo: 100%
  checks:
    nodeStartupTimeoutSeconds: 30
    unhealthyNodeConditions:
      - status: "False"
        timeoutSeconds: 10
        type: e2e.remediation.condition
  selector:
    matchLabels:
      cluster.x-k8s.io/control-plane: ""
      mhc-test: fail
